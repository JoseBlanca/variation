
New class oriented desing


# parallel hdf5 does not work with compressed files
# if you have to do it in parallel in one node is because the computation
# is hard, so io is not the limiting factor, so just use processing.map
# parallel hdf5 only makes sense if you have several nodes


parser = vcf.VcfParser('hola.vcf')
vcfh5 = vcfh5.VcfH5('hola.hdf5')
vcfh5.get_vars_from_vcf(parser)
    # only can be called in an empty VcfH5

in_vcf = vcfh5.VcfH5('hola.hdf5')
chunks = in_vcf.chunks
mafs = map(vcfh5.calculate_maf, chunks)
maf_dset = Dset('/maf')
maf_dset.extend(mafs)


def vcfh5.calculate_maf(dsets_chunk):
    dset_chunk = _get_dset('GT')
    return array.stats.calculate_maf(dset_chunk)

in_vcf = vcfh5.VcfH5('hola.hdf5')
in_vcf.set_meta_only_first_dset = True
chunks = in_vcf.chunks  # dict of dsets
out_vcf = vcfh5.VcfH5('filtered.hdf5')
maf_filter = vcfh5.filters.MafFilter(min_maf=0.5)
chunks = filter(maf_filter, chunks)
out_vcf.extend(chunks)

__init__.py
# global vars an basic config

vcf.py
class VcfParser

vcfh5/
__init__.py
class VcfH5

stats.py
calculate_maf

filters.py
MafFilter

array/
stats.py
calculate_maf

var_dset/
__init__.py
class VarDset
    .meta
    .data

class VarDset(h5py.dset)
    .meta

-----------------------------------------------------

Store the is_phased in the vcf reader

Could we create pipelines with map and filter functions?
We would had an stream of chunks. A chunk is just a numpy.array.
We would need a read_chunks(input_dataset) function that would yield chunks
We would need a write_chunks(output_dataset, chunks)

implement mapping function
map(chunk, function)
The function should take an element of the first axis and return an element to be stored in a hdf5
The mapping function will iterate over the first axis of the hdf5 dataset to avoid having it all in memory
The size of the output dataset will be deduced from the shape of the result of applying the function to the first row of the input matrix.
Take a look at numpy.apply_along_axis, but its a python for, so it is pretty slow.
A slice of a numpy.array is a numpy.array. A native numpy function will be always much faster than a python function.
Take a look at numpy.einsum

implement filter function
filter(chunk, function)
The function will be applied for every slice of the matrix in the first axes. The function will avoide reading the complete matrix in memory. It will read chunk by chunk.
The shape of the output dataset will be the same as the input, but it will have fewer lines.
To filter several datasets at the same time we would filter (chunk, other_chunks) instead of just chunk and the filter function would have to take into account the chunk to calculate the filtering and then apply it to the chunk and the list of other_chunks.

implement select regions function
select_region(index, calldata_or_variants, output_call_data_or_variants)
The index can be a dict[chrom, pos] = index_in_calldata_matrix.
For each region the function will look for the start and the stop in the dict and with that index will load the calldata_input_matrix and write the calldata_output_matrix)
It would yield the chunk for every region



